---
id: strings-l3-tokenize-and-parse-lite
title: "Tokenize & Parse (Lite)"
topic: strings
level: 3
lesson: 3.6
prereqs:
  - strings-l3-regex-basics-flags
  - strings-l3-regex-groups-capture-and-backrefs
  - strings-l3-regex-lookaround-alternation-quantifiers
  - strings-l2-string-iteration-patterns
outcomes:
  - Build a tiny **tokenizer** using **sticky (`y`)** regexes that never “skip ahead”
  - Design **token specs** (order, longest-first) with `skip` and `value` mappers
  - "Parse common formats without a full parser: **CSV-lite**, **key=value lists**, and **simple arithmetic**"
  - Bridge token **unit indices** to **grapheme-aware** ranges for UI (recap)
  - "Recognize pitfalls: catastrophic backtracking, ambiguous alternation, and missing `y`"
tags: ["strings","regex","tokenize","sticky-flag","csv","parsing","shunting-yard","lookaround"]
est_minutes: 55
concepts_introduced:
  - strings.tokenize.sticky_engine
  - strings.parse.csv_lite
  - strings.parse.shunting_yard
concepts_reused:
  - strings.regex.flags
  - strings.regex.lookaround
  - strings.unicode.iteration.generators
crosslinks:
  - { to: strings-l2-string-iteration-patterns, why: "Map code-unit token ranges to grapheme boundaries for UI" }
  - { to: strings-l3-replace-replaceall, why: "Escaping and map-based replacements for pre/post processing" }
  - { to: strings-l3-regex-groups-capture-and-backrefs, why: "Groups and named captures inspire token specs" }
checks:
  - type: quiz
    id: strings-l3-tokenize-and-parse-lite-quiz
  - type: unit
    entry: tokenizer.ts
    tests: tokenizer.test.ts
  - type: unit
    entry: csvLiteParse.ts
    tests: csvLiteParse.test.ts
  - type: unit
    entry: kvScan.ts
    tests: kvScan.test.ts
  - type: unit
    entry: arithEval.ts
    tests: arithEval.test.ts
  - type: unit
    entry: mapTokenRangeToGrapheme.ts
    tests: mapTokenRangeToGrapheme.test.ts
---

## Why this matters

You don’t need a full parser generator to tackle everyday text: logs, CSVish rows, `key=value` configs, or tiny expressions. A **sticky-regex tokenizer** plus a few simple passes can take you a long way—deterministic, fast, and easy to reason about.

> Sticky (`y`) matters: the engine **must** match at `lastIndex`. If it can’t, you fail fast rather than silently skipping text like `g` can.

---

## A minimal tokenizer engine

### Token spec

```ts
export type Token<T extends string = string> = {
  type: T;
  text: string;
  start: number; // unit indices
  end: number;
  value?: unknown; // optional mapped value
};

export type TokenSpec<T extends string = string> = {
  type: T;
  re: RegExp;             // MUST include 'y' (and usually 'u')
  skip?: boolean;         // whitespace/comments, etc.
  map?: (text: string) => unknown; // value mapper, e.g., Number
};
````

### Engine (sticky scan)

```ts
function ensureSticky(re: RegExp): RegExp {
  const flags = re.flags.includes("y") ? re.flags : re.flags + "y";
  return new RegExp(re.source, flags);
}

export function* tokenize<T extends string>(input: string, specs: TokenSpec<T>[]): Generator<Token<T>> {
  // Order matters — put longer/specific patterns FIRST
  const rules = specs.map(s => ({ ...s, re: ensureSticky(s.re) }));
  let i = 0;
  while (i < input.length) {
    let matched = false;
    for (const s of rules) {
      s.re.lastIndex = i;
      const m = s.re.exec(input);
      if (!m) continue;
      matched = true;
      const text = m[0];
      const tok: Token<T> = { type: s.type, text, start: i, end: i + text.length };
      if (!s.skip) {
        if (s.map) tok.value = s.map(text);
        yield tok;
      }
      i = tok.end;
      break;
    }
    if (!matched) {
      throw new Error(`Unexpected character at ${i}: ${JSON.stringify(input[i])}`);
    }
  }
}
```

**Guidelines**

* Specs should be **longest-first** to avoid prefix shadowing (`>=` before `>`).
* Prefer **tempered classes** (`[^"]*`) over `.*?` where possible to limit backtracking.
* Add a `WHITESPACE` spec with `skip:true`.

---

## Example 1 — CSV-lite (RFC4180-ish, one line)

Rules (single line):

* Fields separated by a delimiter (default `,`).
* Unquoted fields: any chars except delimiter or newline.
* Quoted fields: `"` … `"` with `""` as escaped quote.
* Optional spaces around delimiters are **data** unless you choose to trim.

```ts
export function csvLiteParse(line: string, delim = ","): string[] {
  const D = delim.replace(/[.*+?^${}()|[\]\\]/g, "\\$&");
  const specs: TokenSpec<"Q"|"QQ"|"U"|"D"|"E">[] = [
    { type: "Q",  re: /"(?:[^"\r\n]|"")*"/yu },      // quoted field
    { type: "U",  re: new RegExp(`[^"${D}\\r\\n]*`, "yu") }, // unquoted run (may be empty)
    { type: "D",  re: new RegExp(D, "yu") },         // delimiter
    { type: "E",  re: /\r?\n/yu },                   // enders (shouldn't occur for single line)
  ];

  const fields: string[] = [];
  let current = "";
  let expectField = true;

  for (const tok of tokenize(line, specs)) {
    if (tok.type === "D") { fields.push(current); current = ""; expectField = true; continue; }
    if (tok.type === "Q") {
      if (!expectField) current += tok.text; // stray quote sequence inside? treat as text
      else {
        const inner = tok.text.slice(1, -1).replace(/""/g, `"`);
        current += inner;
        expectField = false;
      }
      continue;
    }
    if (tok.type === "U") { current += tok.text; expectField = false; continue; }
    if (tok.type === "E") break;
  }
  fields.push(current);
  return fields;
}
```

*Notes:* This “lite” version doesn’t support embedded newlines (multi-line fields). That’s usually OK for problem sets and many logs. You can extend with a state machine if needed.

---

## Example 2 — key=value scanner (spaces allowed, quotes or bare)

Use a sticky scanner to split `key=value` pairs where `value` is either quoted (`"..."` or `'...'` with simple escapes) or a bare run to the next separator.

```ts
export type KV = { key: string; value: string };

export function kvScan(s: string): KV[] {
  const specs: TokenSpec<"WS"|"KEY"|"EQ"|"QVAL"|"VAL"|"SEP">[] = [
    { type: "WS",   re: /\s+/yu, skip: true },
    { type: "KEY",  re: /[A-Za-z_]\w*/yu },
    { type: "EQ",   re: /=/yu },
    { type: "QVAL", re: /"(?:[^"\\]|\\.)*"|'(?:[^'\\]|\\.)*'/yu },
    { type: "VAL",  re: /[^,;]+/yu },
    { type: "SEP",  re: /[,;]/yu }
  ];

  const out: KV[] = [];
  const it = tokenize(s, specs);
  for (let t = it.next(); !t.done; t = it.next()) {
    if (t.value.type !== "KEY") throw new Error(`Expected key at ${t.value.start}`);
    const key = t.value.text;
    const eq = it.next().value; if (!eq || eq.type !== "EQ") throw new Error(`Expected '=' after ${key}`);
    const valTok = it.next().value;
    if (!valTok || (valTok.type !== "QVAL" && valTok.type !== "VAL")) throw new Error(`Expected value for ${key}`);
    let value = valTok.text;
    if (valTok.type === "QVAL") {
      const q = value[0]; value = value.slice(1, -1).replace(/\\(["'\\nrt])/g, (_m, c) =>
        c === "n" ? "\n" : c === "r" ? "\r" : c === "t" ? "\t" : c);
    } else {
      value = value.trim();
    }
    out.push({ key, value });
    // Consume optional separator
    const peek = it.next();
    if (!peek.done && peek.value.type !== "SEP") { // put it back (simple approach)
      // In generator form we can’t "unread"; in tests, feed exact formats or wrap in a controller.
      // For simplicity, require a separator or end.
      throw new Error(`Expected separator after ${key}`);
    }
  }
  return out;
}
```

> If you want a friendlier API, wrap `tokenize` with a small controller that supports **peek** and **unread**.

---

## Example 3 — simple arithmetic (shunting-yard)

Tokenize `NUMBER ( ) + - * /` and evaluate with precedence.

```ts
type TT = "WS"|"NUM"|"LP"|"RP"|"PLUS"|"MIN"|"MUL"|"DIV";

const ARITH_SPECS: TokenSpec<TT>[] = [
  { type: "WS",  re: /\s+/yu, skip: true },
  { type: "NUM", re: /(?:\d+(?:_\d+)*)(?:\.\d+)?/yu, map: s => Number(s.replace(/_/g, "")) },
  { type: "LP",  re: /\(/yu }, { type: "RP", re: /\)/yu },
  { type: "PLUS",re: /\+/yu }, { type: "MIN", re: /-/yu },
  { type: "MUL", re: /\*/yu }, { type: "DIV", re: /\//yu },
];

export function arithEval(expr: string): number {
  const outVal: number[] = [];
  const ops: TT[] = [];
  const prec: Record<TT, number> = { PLUS:1, MIN:1, MUL:2, DIV:2 } as any;

  const apply = () => {
    const op = ops.pop()!;
    const b = outVal.pop()!, a = outVal.pop()!;
    outVal.push(op==="PLUS"?a+b:op==="MIN"?a-b:op==="MUL"?a*b:a/b);
  };

  for (const tok of tokenize(expr, ARITH_SPECS)) {
    if (tok.type === "NUM") outVal.push(tok.value as number);
    else if (tok.type === "LP") ops.push(tok.type);
    else if (tok.type === "RP") {
      while (ops.length && ops[ops.length-1] !== "LP") apply();
      if (!ops.length) throw new Error("Unbalanced ')'");
      ops.pop(); // pop LP
    } else { // operator
      while (ops.length && (ops[ops.length-1] in prec) && prec[ops[ops.length-1]] >= prec[tok.type]) apply();
      ops.push(tok.type);
    }
  }
  while (ops.length) {
    if (ops[ops.length-1] === "LP") throw new Error("Unbalanced '('");
    apply();
  }
  if (outVal.length !== 1) throw new Error("Bad expression");
  return outVal[0]!;
}
```

---

## Bridging to grapheme-safe UI

Token `start/end` are **code-unit** indices. For carets/highlights in UI, **snap** to grapheme boundaries (see L2):

```ts
import { iterGraphemes } from "./iterGraphemes";

export function mapTokenRangeToGrapheme(s: string, start: number, end: number): { start: number; end: number } {
  const bounds = Array.from(iterGraphemes(s), g => g.start).concat([s.length]).sort((a,b)=>a-b);
  const snap = (i: number, dir: "left"|"right") => {
    let k = 0; while (k < bounds.length && bounds[k] < i) k++;
    return dir === "left" ? bounds[Math.max(0, k - (bounds[k] === i ? 0 : 1))] : bounds[Math.min(bounds.length-1, k)];
  };
  return { start: snap(start, "left"), end: snap(end, "right") };
}
```

---

## Pitfalls

* **Forgot `y`**: your tokenizer might silently skip text because `/g` advances to the next match.
* **Spec order**: put **longer/more specific** first to avoid partial matches (e.g., `>=` before `>`).
* **Greedy patterns**: prefer explicit classes (`[^"]*`) over `.*` and consider **tempered** constructs to avoid backtracking.
* **Indices vs UI**: token ranges are units; snap to grapheme boundaries for display.

---

## Exercises

### 1) `tokenizer.ts` — productionize the engine

Add a thin controller around `tokenize` that supports `peek()` and `unread(tok)` for one-token pushback.

**Starter — `tokenizer.ts`**

```ts
export * from "./_engine"; // your tokenize & types here

export class Scanner<T extends string> {
  private buf: Token<T>[] = [];
  private it: Generator<Token<T>>;
  constructor(input: string, specs: TokenSpec<T>[]) { this.it = tokenize(input, specs); }
  next(): Token<T> | null { if (this.buf.length) return this.buf.pop()!; const n = this.it.next(); return n.done ? null : n.value; }
  peek(): Token<T> | null { const n = this.next(); if (n) this.unread(n); return n; }
  unread(tok: Token<T>) { this.buf.push(tok); }
}
```

---

### 2) `csvLiteParse.ts` — robust CSV-lite

Finish/adjust the CSV parser to:

* support custom delimiter,
* accept empty fields,
* treat surrounding spaces inside quotes as data,
* and throw on unmatched quotes.

*(Use the example implementation as a baseline; add an unmatched-quote check by scanning for a closing `"`.)*

---

### 3) `kvScan.ts` — tolerant `key=value` list

Using the scanner, parse `key=value` pairs separated by `,`/`;`, allowing quoted values with `\"` `\'` `\\` escapes and bare values otherwise. Return an ordered array of `{key,value}`.

**Starter — `kvScan.ts`**

```ts
export type KV = { key: string; value: string };
// Implement with Scanner; accept trailing separator; trim bare values.
```

---

### 4) `arithEval.ts` — extend arithmetic

Extend to support unary `-` (negation) and precedence with right-associative exponent `^`. Reuse sticky tokenizer, add a precedence table, and adjust the shunting-yard.

**Starter — `arithEval.ts`**

```ts
// Add TT:"POW" for ^ and handle unary MIN with a state flag (prev token type)
```

---

### 5) `mapTokenRangeToGrapheme.ts` — UI bridge

Implement the `mapTokenRangeToGrapheme` helper using `iterGraphemes` from L2. Test with emoji + modifiers.

---

## Quiz (checks)

1. The **sticky** flag `y` makes a regex:
   A) Scan ahead to the next match automatically
   B) **Match only at `lastIndex` (no skipping)** ✅
   C) Ignore anchors
   D) Unicode-aware

2. Spec order in a tokenizer should be:
   A) Random
   B) Alphabetical
   C) **Longest/specific first to avoid prefix shadowing** ✅
   D) Shortest first

3. CSV-lite quoted fields usually escape `"` as:
   A) `\"`
   B) **`""`** ✅
   C) `&quot;`
   D) `^"`

4. Token `start/end` indices are in:
   A) Grapheme clusters
   B) Code points
   C) **UTF-16 code units** ✅
   D) Bytes

5. To avoid catastrophic backtracking in token patterns, prefer:
   A) `.*` everywhere
   B) **Explicit negated classes/tempered dots** ✅
   C) Lookbehind for everything
   D) Removing the `u` flag

---

## Takeaways

* A tiny **sticky-regex** tokenizer is enough for many tasks: CSV-lite, `key=value` lists, and small expression grammars.
* Order your specs carefully; design patterns to be **linear-time**.
* Token indices are **code units**; snap to **graphemes** for UI.
* When complexity grows, you can still layer simple passes (tokenize → transform → join) before reaching for heavy parsers.

---

## What’s next

Next: **Strings L3 — Case Folding & Simple Normalization** to build locale-aware, accent-insensitive search pipelines that compose with your tokenizers.
