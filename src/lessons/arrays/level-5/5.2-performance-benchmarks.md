---
id: arrays-l5-performance-benchmarks
title: "Performance Benchmarks (chain vs fuse, loops vs methods, mutate vs copy)"
topic: arrays
level: 5
lesson: 5.2
prereqs:
  - arrays-l2-map-filter-reduce-flat-flatMap
  - arrays-l3-immutability-patterns-and-costs
  - arrays-l3-in-place-ops-two-pointers
  - arrays-l2-utilities-and-misc
outcomes:
  - Design trustworthy microbenchmarks (warm-up, steady-state, verify-equal-results)
  - Compare common choices: chained methods vs single-pass, `for` vs `map/reduce`, `push` vs `concat/spread`, `splice` vs `toSpliced`, `reverse` vs `toReversed`, `sort` vs `toSorted`
  - Interpret results and prefer clarity unless profiling shows a real win
  - Build a tiny, reusable Node/TS benchmark harness
tags: ["arrays","performance","benchmark","microbench","immutability","methods","loops"]
est_minutes: 50
checks:
  - type: quiz
    id: arrays-l5-bench-quiz
  - type: unit
    entry: benchHarness.ts
    tests: benchHarness.test.ts
  - type: unit
    entry: chainVsReduce.bench.ts
    tests: chainVsReduce.test.ts
  - type: unit
    entry: pushConcat.bench.ts
    tests: pushConcat.test.ts
  - type: unit
    entry: spliceToSpliced.bench.ts
    tests: spliceToSpliced.test.ts
  - type: unit
    entry: reverseToReversed.bench.ts
    tests: reverseToReversed.test.ts
---

## Why this matters

Performance debates are noisy. A **tiny, repeatable harness** lets you test *your actual data and environment*, compare alternatives fairly, and then choose **clarity by default**—or speed when it’s truly worth it.

---

## Microbenchmark sanity checklist

- **Warm up** first (JIT + inline caches).  
- **Steady state**: run each candidate for N milliseconds (not fixed iterations).  
- **Verify results**: compute a cheap checksum so both variants do real work.  
- **Randomize order** of candidates between samples.  
- **Avoid I/O/console** inside timed regions.  
- **Use big enough inputs** to drown overhead; scale `n` up.  
- **Report ops/sec & relative %** with variance across samples.

> Microbenches are fickle. Treat results as **signals**, not absolute truths. Always re-test in production-like builds.

---

## Our tiny harness (Node + TS)

**Starter — `benchHarness.ts`**
```ts
import { performance } from "node:perf_hooks";

export type BenchCase<TSetup, TResult> = {
  name: string;
  setup: () => TSetup;             // fresh data per run (avoid cross-run pollution)
  run: (data: TSetup) => TResult;  // the thing we time
  verify?: (result: TResult) => void; // optional invariant/assertion
};

export type BenchOptions = {
  warmupMs?: number;
  timeMs?: number;
  samples?: number;                // independent timing runs per case
  gcBetween?: boolean;             // call global.gc() if enabled with --expose-gc
};

export type BenchStat = {
  name: string;
  opsPerSec: number;
  iters: number;
  elapsedMs: number;
};

export type BenchResult = {
  case: string;
  stats: BenchStat[];
  meanOpsPerSec: number;
};

function nowMs(): number { return performance.now(); }

function shuffle<T>(xs: T[]): T[] {
  for (let i = xs.length - 1; i > 0; i--) {
    const j = (Math.random() * (i + 1)) | 0;
    [xs[i], xs[j]] = [xs[j], xs[i]];
  }
  return xs;
}

function blackhole(x: unknown): number {
  // Cheap checksum to prevent dead-code elimination
  // Number coercion keeps it fast and side-effect-free
  return typeof x === "number" ? x : (x ? 1 : 0);
}

export async function runBench<TSetup, TResult>(
  cases: Array<BenchCase<TSetup, TResult>>,
  opts: BenchOptions = {}
): Promise<BenchResult[]> {
  const warmupMs = opts.warmupMs ?? 500;
  const timeMs = opts.timeMs ?? 750;
  const samples = opts.samples ?? 5;

  const results: BenchResult[] = [];

  for (const c of cases) {
    const stats: BenchStat[] = [];

    // Warm-up
    {
      const data = c.setup();
      const endAt = nowMs() + warmupMs;
      let iters = 0;
      while (nowMs() < endAt) {
        const r = c.run(data);
        if (c.verify) c.verify(r);
        blackhole(r as unknown);
        iters++;
      }
      if (opts.gcBetween && typeof global.gc === "function") global.gc();
    }

    // Timed samples
    for (let s = 0; s < samples; s++) {
      const data = c.setup();
      const endAt = nowMs() + timeMs;
      let iters = 0;
      const t0 = nowMs();
      while (nowMs() < endAt) {
        const r = c.run(data);
        if (c.verify) c.verify(r);
        blackhole(r as unknown);
        iters++;
      }
      const elapsed = nowMs() - t0;
      stats.push({
        name: `${c.name}#${s + 1}`,
        opsPerSec: (iters / elapsed) * 1000,
        iters,
        elapsedMs: elapsed,
      });
      if (opts.gcBetween && typeof global.gc === "function") global.gc();
    }

    // Aggregate
    const mean =
      stats.reduce((a, b) => a + b.opsPerSec, 0) / Math.max(1, stats.length);

    results.push({ case: c.name, stats, meanOpsPerSec: mean });
  }

  // Randomize case order to reduce bias when run repeatedly
  return shuffle(results);
}
````

---

## What to compare

### A) Chained methods vs single-pass `for`

* Task: “map → filter → map” pipeline vs fused one-pass implementation.
* Expectation: fused is often a bit faster; not always enough to trade readability.

**Starter — `chainVsReduce.bench.ts`**

```ts
import { runBench } from "./benchHarness";

type Data = number[];
const N = 100_000;

const setup = (): Data => Array.from({ length: N }, (_, i) => (i % 97) - 48);

const chainCase = {
  name: "chain: map->filter->map",
  setup,
  run: (xs: Data) =>
    xs.map(x => x + 1).filter(x => x % 3 === 0).map(x => x * 2),
  verify: (out: number[]) => { if (!Array.isArray(out)) throw new Error(); }
};

const fusedCase = {
  name: "fused: single pass",
  setup,
  run: (xs: Data) => {
    const out: number[] = [];
    for (let i = 0; i < xs.length; i++) {
      const a = xs[i] + 1;
      if (a % 3 === 0) out.push(a * 2);
    }
    return out;
  },
  verify: chainCase.verify
};

(async () => {
  const res = await runBench([chainCase, fusedCase], { samples: 5, timeMs: 800 });
  console.table(res.map(r => ({ case: r.case, opsPerSec: Math.round(r.meanOpsPerSec) })));
})();
```

---

### B) `push` vs `concat` vs spread for append

* Task: append `m` items to an existing array.
* Expectation: `push` in a loop is typically faster and lighter than repeated `concat`/spreads.

**Starter — `pushConcat.bench.ts`**

```ts
import { runBench } from "./benchHarness";

const BASE = 50_000, M = 500;

const baseSetup = () => Array.from({ length: BASE }, (_, i) => i);

export default (async () => {
  const pushCase = {
    name: "append: push loop",
    setup: baseSetup,
    run: (xs: number[]) => {
      const out = xs.slice();
      for (let i = 0; i < M; i++) out.push(i);
      return out;
    }
  };

  const concatCase = {
    name: "append: concat once",
    setup: baseSetup,
    run: (xs: number[]) => xs.concat(Array.from({ length: M }, (_, i) => i))
  };

  const spreadCase = {
    name: "append: spread once",
    setup: baseSetup,
    run: (xs: number[]) => [...xs, ...Array.from({ length: M }, (_, i) => i)]
  };

  const res = await runBench([pushCase, concatCase, spreadCase], { samples: 5 });
  console.table(res.map(r => ({ case: r.case, opsPerSec: Math.round(r.meanOpsPerSec) })));
})();
```

---

### C) `splice` (mutate) vs `toSpliced` (immutable)

* Task: remove/insert a middle block.
* Expectation: `toSpliced` pays for copying; `splice` edits in place. Prefer `toSpliced` for clarity/immutability unless hot.

**Starter — `spliceToSpliced.bench.ts`**

```ts
import { runBench } from "./benchHarness";

const N = 80_000;
const setup = () => Array.from({ length: N }, (_, i) => i);

const mutate = {
  name: "splice (mutate)",
  setup,
  run: (xs: number[]) => {
    const a = xs.slice();         // fair: copy before mutating
    a.splice(30_000, 5_000, -1, -2, -3);
    return a;
  }
};

const immutable = {
  name: "toSpliced (copy-by-change)",
  setup,
  run: (xs: number[]) => xs.toSpliced(30_000, 5_000, -1, -2, -3)
};

(async () => {
  const res = await runBench([mutate, immutable], { samples: 5 });
  console.table(res.map(r => ({ case: r.case, opsPerSec: Math.round(r.meanOpsPerSec) })));
})();
```

---

### D) `reverse` vs `toReversed`

* Expectation: `reverse` is in-place; `toReversed` allocates and copies.

**Starter — `reverseToReversed.bench.ts`**

```ts
import { runBench } from "./benchHarness";

const N = 120_000;
const setup = () => Array.from({ length: N }, (_, i) => i);

const inPlace = {
  name: "reverse (mutate)",
  setup,
  run: (xs: number[]) => { const a = xs.slice(); a.reverse(); return a; }
};

const copyByChange = {
  name: "toReversed (copy)",
  setup,
  run: (xs: number[]) => xs.toReversed()
};

(async () => {
  const res = await runBench([inPlace, copyByChange], { samples: 5 });
  console.table(res.map(r => ({ case: r.case, opsPerSec: Math.round(r.meanOpsPerSec) })));
})();
```

---

### E) Bonus idea: `sort` vs `toSorted`

Functionally the same ordering cost (**O(n log n)** comparisons). `toSorted` adds a copy; `sort` mutates. Benchmark if your dataset is large and you care.

---

## Reading results (and deciding)

* If the “faster” option is ≤10–20% improvement **and** less readable, prefer the clear one.
* If a hot path shows **material** wins (e.g., 2× on big data), consider the faster version and encapsulate it behind a friendly API.

---

## Common pitfalls

* Comparing **mutating** vs **immutable** without making a **fair copy** first (gives mutating an unfair advantage if it reuses the same array across iterations).
* Doing **work in setup** for one case but not the other. Keep setups symmetric.
* Measuring **tiny inputs** (noise dominates).
* Forgetting `node --expose-gc` if you use `gcBetween: true`.

---

## Exercises

### 1) Finish the harness

* Ensure `benchHarness.ts` compiles and exports `runBench`.
* Add a helper `formatTable(results)` to pretty-print mean ops/sec and % vs best.

**Tests** check: output shape (not absolute numbers), stable keys, `samples` honored.

---

### 2) Chain vs fused

Complete `chainVsReduce.bench.ts`. Add an assertion that both variants return arrays of **equal length** for the same input.

---

### 3) Push vs concat vs spread

Complete `pushConcat.bench.ts`. Ensure each variant returns the same last 3 elements.

---

### 4) Splice vs toSpliced

Complete `spliceToSpliced.bench.ts`. Verify the edited window matches `[-1,-2,-3]` at the start of the insertion point.

---

### 5) Reverse vs toReversed

Complete `reverseToReversed.bench.ts`. Verify first/last are swapped in results.

---

## Quiz (checks)

1. Why warm-up before timing?
   A) To allocate arrays earlier
   B) **To let the JIT optimize and reach steady state** ✅
   C) To prefill caches permanently
   D) It doesn’t matter

2. What prevents dead-code elimination in our harness?
   A) `console.log` inside the loop
   B) A try/catch around the loop
   C) **Consuming results via a checksum/blackhole** ✅
   D) Using `var` instead of `let`

3. The fairest way to compare mutate vs immutable is to:
   A) Reuse the same array for mutate, and a fresh one for immutable
   B) **Give mutate a fresh copy too (copy before mutating)** ✅
   C) Subtract copy time from mutate only
   D) Ignore copying—mutate is always faster

4. Reporting **ops/sec** over a time budget (e.g., 750ms) instead of fixed iterations helps because:
   A) It’s easier to code
   B) **It stabilizes results across machines & avoids unlucky small sample effects** ✅
   C) It makes everything faster
   D) It warms up more

5. If two approaches differ by \~10% and the slower one is much clearer, you should usually:
   A) Pick the faster one
   B) **Pick the clearer one** ✅
   C) Rewrite both in WebAssembly
   D) Throw an error at runtime

---

## Takeaways

* Build small, honest benchmarks: **warm up**, **verify**, **time windows**, **no I/O**, **big inputs**.
* Expect mutating ops to avoid copy costs; immutable ops pay for clarity with allocation.
* Many “micro wins” don’t matter—optimize only where it **moves the needle**.
* Encapsulate the fast path behind a tidy function so your codebase stays readable.

---

## What’s next

Onward to **Arrays L5 — Large Data & Iterators** to stream and chunk big arrays, introduce typed arrays, and design memory-aware pipelines.