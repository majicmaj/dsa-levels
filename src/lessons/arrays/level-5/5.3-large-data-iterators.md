---
id: arrays-l5-large-data-iterators
title: "Large Data & Iterators (lazy pipelines, async, typed arrays)"
topic: arrays
level: 5
lesson: 5.3
prereqs:
  - arrays-l2-map-filter-reduce-flat-flatMap
  - arrays-l3-immutability-patterns-and-costs
  - arrays-l4-sliding-window-katas
  - arrays-l5-typescript-advanced
outcomes:
  - "Build **lazy** pipelines with (Async)Iterables: `map`, `filter`, `take`, `chunksOf`"
  - Convert between arrays and iterables safely; know when to stay lazy vs materialize
  - Stream data with **async generators** and manage backpressure by `await`ing
  - Use **TypedArrays** (`Float32Array`, `Int32Array`, etc.) for numeric workloads and zero-copy tricks
  - Implement memory-aware transforms (subarray views, batched processing, windowing) with clear Big-O & allocation costs
tags: ["arrays","iterable","generator","async-iterator","streams","typedarray","memory","performance"]
est_minutes: 55
checks:
  - type: quiz
    id: arrays-l5-large-iterators-quiz
  - type: unit
    entry: iter_map_filter_take.ts
    tests: iter_map_filter_take.test.ts
  - type: unit
    entry: chunksOf.ts
    tests: chunksOf.test.ts
  - type: unit
    entry: asyncBatch.ts
    tests: asyncBatch.test.ts
  - type: unit
    entry: sumFloat64.ts
    tests: sumFloat64.test.ts
  - type: unit
    entry: normalizeFloat32.ts
    tests: normalizeFloat32.test.ts
  - type: unit
    entry: reshape2D.ts
    tests: reshape2D.test.ts
---

## Why this matters

Arrays are great—until your data doesn’t fit in memory, arrives over time, or you need tight numeric loops. **Iterables** let you compute **on demand** (constant memory), and **TypedArrays** give you raw, cache-friendly numeric storage. Together they unlock fast, scalable pipelines.

---

## Mental model

- **Iterable** = something you can `for…of`. It produces values **lazily** via `next()`.
- **AsyncIterable** = values arrive over time; you `for await…of`.
- **Generator functions** (`function*`) produce iterables; **async generators** (`async function*`) produce async iterables.
- **TypedArrays** store packed numeric data in an **ArrayBuffer**. Many operations (slice/subarray/set) are **views** (zero-copy).

**Key trade-off:** Materializing with `[...]` or `Array.from()` is eager (allocates). Staying iterable is lazy (memory-light) but you can consume only once.

---

## Lazy building blocks

### Synchronous
```ts
export function* mapIter<A, B>(it: Iterable<A>, fn: (a: A, i: number) => B): Iterable<B> {
  let i = 0;
  for (const a of it) yield fn(a, i++);
}

export function* filterIter<A>(it: Iterable<A>, pred: (a: A, i: number) => boolean): Iterable<A> {
  let i = 0;
  for (const a of it) if (pred(a, i++)) yield a;
}

export function* take<A>(it: Iterable<A>, n: number): Iterable<A> {
  if (n <= 0) return;
  let i = 0;
  for (const a of it) { yield a; if (++i >= n) break; }
}

export function* chunksOf<A>(it: Iterable<A>, size: number): Iterable<A[]> {
  if (size <= 0) return;
  let buf: A[] = [];
  for (const a of it) {
    buf.push(a);
    if (buf.length === size) { yield buf; buf = []; }
  }
  if (buf.length) yield buf;
}
````

Usage:

```ts
const src = [1,2,3,4,5,6];
const pipeline = take(mapIter(filterIter(src, x => x%2===0), x => x*x), 2);
[...pipeline]; // [4, 16]
```

### Asynchronous

```ts
export async function* mapAsync<A, B>(
  it: AsyncIterable<A>, fn: (a: A, i: number) => Promise<B> | B
): AsyncIterable<B> {
  let i = 0;
  for await (const a of it) yield await fn(a, i++);
}

export async function* asyncChunksOf<A>(
  it: AsyncIterable<A>, size: number
): AsyncIterable<A[]> {
  if (size <= 0) return;
  let buf: A[] = [];
  for await (const a of it) {
    buf.push(a);
    if (buf.length === size) { yield buf; buf = []; }
  }
  if (buf.length) yield buf;
}
```

> **Backpressure:** `for await` pulls the next value only after the previous one is processed. Don’t buffer unless you need to.

---

## Arrays ↔ Iterables (when to materialize)

* **Stay lazy** while filtering/mapping large streams.
* **Materialize** at UI/API boundaries (e.g., need random access, length, or to sort).

```ts
const iter = mapIter(range(1_000_000), x => x+1); // lazy
const first100 = [...take(iter, 100)];            // materialize the first 100 only
```

Tiny `range`:

```ts
function* range(n: number, start = 0): Iterable<number> {
  for (let i = 0; i < n; i++) yield start + i;
}
```

---

## TypedArrays: fast numeric storage

* Families: `Int8Array`, `Uint8Array`, `Int16Array`, `Int32Array`, `Float32Array`, `Float64Array`, etc.
* **Backed by** an `ArrayBuffer`. `subarray` creates a **view** (no copy). `slice` **copies**.
* **Bulk copy**: `dst.set(src, offset)` uses memmove-like paths (fast).

Examples:

```ts
// Sum Float64
export function sumFloat64(a: Float64Array): number {
  let s = 0;
  for (let i = 0; i < a.length; i++) s += a[i];
  return s;
}

// Normalize Float32 to [0,1] in-place (guard div-by-zero)
export function normalizeFloat32(a: Float32Array): Float32Array {
  let min = Infinity, max = -Infinity;
  for (let i = 0; i < a.length; i++) { const v = a[i]; if (v < min) min = v; if (v > max) max = v; }
  const span = max - min || 1;
  for (let i = 0; i < a.length; i++) a[i] = (a[i] - min) / span;
  return a;
}

// 2D reshape view (no copy): array of row views via subarray
export function reshape2D(buf: Float32Array, rows: number, cols: number): Float32Array[] {
  if (rows * cols !== buf.length) throw new RangeError("mismatch");
  const out: Float32Array[] = [];
  for (let r = 0; r < rows; r++) out.push(buf.subarray(r * cols, (r + 1) * cols));
  return out;
}
```

> **Zero-copy tricks:** prefer `subarray` over `slice`; use `set` to stitch TypedArrays; avoid converting to `number[]` unless necessary.

---

## Walkthrough: streaming + batching

**Goal:** Process a large (async) source of numbers, keep those ≥ 0, square them, and compute the sum **in batches** of 1k to bound memory.

```ts
async function* source(): AsyncIterable<number> {
  for (let i = 0; i < 1_000_000; i++) { // pretend these come from the network
    yield (i % 7) - 3;                  // some negatives
    await Promise.resolve();            // simulate async boundary
  }
}

async function process(): Promise<number> {
  let total = 0;
  const positives = mapAsync(source(), x => (x >= 0 ? x : null));
  const clean = (async function* () {
    for await (const v of positives) if (v !== null) yield v * v;
  })();

  for await (const batch of asyncChunksOf(clean, 1000)) {
    total += batch.reduce((s, x) => s + x, 0);
  }
  return total;
}
```

* Constant memory (bounded by batch size).
* Backpressure via `for await`: next batch only pulled after reduction.

---

## Common pitfalls

* **Accidental eager materialization**: `Array.from(iter)` or `[...iter]` on huge streams explodes memory.
* **Reusing an iterator**: most generators are **single-use**; create fresh ones for multiple passes.
* **TypedArray confusion**: `slice` **copies**, `subarray` **views**; know which you need.
* **Mixed number types**: converting large `number[]` to `Float32Array` can lose precision; pick the right type.
* **Async pipelines**: forgetting `await` inside transforms can produce unresolved Promises.

---

## Exercises

### 1) `iter_map_filter_take` — compose lazily

Implement a tiny pipe that mirrors `xs.map(...).filter(...).slice(0,n)` **lazily**.

**Starter — `iter_map_filter_take.ts`**

```ts
export function* mapIter<A, B>(it: Iterable<A>, fn: (a: A, i: number) => B): Iterable<B> {
  // implement
}

export function* filterIter<A>(it: Iterable<A>, pred: (a: A, i: number) => boolean): Iterable<A> {
  // implement
}

export function* take<A>(it: Iterable<A>, n: number): Iterable<A> {
  // implement
}
```

**Tests expect**

* Correct order, short-circuiting at `n`, no precomputation.

---

### 2) `chunksOf` — batch an iterable

**Starter — `chunksOf.ts`**

```ts
export function* chunksOf<A>(it: Iterable<A>, size: number): Iterable<A[]> {
  // yield arrays of length <= size; last can be shorter; size<=0 yields nothing
}
```

---

### 3) `asyncBatch` — batch an AsyncIterable

Build `asyncChunksOf(it, size)` and consume with `for await`.

**Starter — `asyncBatch.ts`**

```ts
export async function* asyncChunksOf<A>(
  it: AsyncIterable<A>, size: number
): AsyncIterable<A[]> {
  // similar to chunksOf but with for await
}
```

---

### 4) `sumFloat64` — TypedArray sum

**Starter — `sumFloat64.ts`**

```ts
export function sumFloat64(a: Float64Array): number {
  // tight loop sum
  return 0;
}
```

---

### 5) `normalizeFloat32` — in-place normalization

**Starter — `normalizeFloat32.ts`**

```ts
export function normalizeFloat32(a: Float32Array): Float32Array {
  // scale to [0,1]; handle constant arrays
  return a;
}
```

---

### 6) `reshape2D` — row views without copies

**Starter — `reshape2D.ts`**

```ts
export function reshape2D(buf: Float32Array, rows: number, cols: number): Float32Array[] {
  // validate rows*cols, return array of subarray views
  return [];
}
```

---

## Quiz (checks)

1. A key advantage of **Iterables** over Arrays for large data is:
   A) Faster sorting
   B) **Lazy consumption with constant memory** ✅
   C) Built-in parallelism
   D) Automatic caching

2. `for await (const x of it)` provides backpressure because:
   A) It buffers all values first
   B) **It pulls the next value only after the previous is processed** ✅
   C) It runs items in parallel
   D) It uses Web Workers

3. In TypedArrays, `subarray(start,end)` …
   A) Copies the data
   B) **Creates a view (zero copy)** ✅
   C) Freezes the buffer
   D) Reallocates a new ArrayBuffer

4. Converting a huge `Iterable` to an array with `[...iter]` is risky because:
   A) Iterables are unordered
   B) **It materializes all values into memory** ✅
   C) It changes value types
   D) It makes it non-iterable

5. Summing a `Float64Array` with a tight loop vs `reduce`:
   A) `reduce` is always faster
   B) Tight loop is always faster
   C) **Tight loop typically avoids callback overhead** ✅
   D) They are identical at the engine level

---

## Takeaways

* Use (Async)Iterables for **lazy, bounded-memory** pipelines; materialize only at the edges.
* Compose tiny generator utilities (`mapIter`, `filterIter`, `take`, `chunksOf`) for clarity.
* TypedArrays + `subarray`/`set` enable **zero-copy** numeric workflows.
* For streams, batching keeps memory flat and leverages **backpressure** naturally.

---

## What’s next

Finish Arrays with **Arrays L5 — Putting It All Together**: a capstone project that combines strings, arrays, Sets/Maps, iterables, typed arrays, and performance testing to build a mini analytics pipeline end-to-end.
