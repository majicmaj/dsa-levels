---
id: arrays-l5-capstone
title: "Capstone: Mini Analytics Pipeline"
topic: arrays
level: 5
lesson: 5.4
prereqs:
  - arrays-l4-groupby-and-frequency
  - arrays-l4-sliding-window-katas
  - arrays-l4-object-map-interop
  - arrays-l4-anagrams-pipeline
  - arrays-l5-large-data-iterators
  - arrays-l5-performance-benchmarks
  - arrays-l5-typescript-advanced
outcomes:
  - Design an end-to-end data pipeline that stays **lazy** as long as possible and materializes only at the edges
  - Combine Arrays with **Set/Map**, **(Async)Iterables**, and **TypedArrays** to compute reports efficiently
  - Implement **dedupe (first/last win)**, **groupBy/countBy**, **Top-K**, and **sliding window** metrics
  - Produce a final **report object** and **entries arrays** suitable for UI tables and charts
  - Benchmark and explain trade-offs (mutate vs copy-by-change, chain vs fuse, object vs Map)
tags: ["arrays","iterables","set","map","typedarray","groupBy","top-k","sliding-window","benchmark","typescript"]
est_minutes: 90
checks:
  - type: quiz
    id: arrays-l5-capstone-quiz
  - type: unit
    entry: capstone/generateData.ts
    tests: capstone/generateData.test.ts
  - type: unit
    entry: capstone/normalize.ts
    tests: capstone/normalize.test.ts
  - type: unit
    entry: capstone/dedupe.ts
    tests: capstone/dedupe.test.ts
  - type: unit
    entry: capstone/metrics.ts
    tests: capstone/metrics.test.ts
  - type: unit
    entry: capstone/report.ts
    tests: capstone/report.test.ts
  - type: unit
    entry: capstone/bench.capstone.ts
    tests: capstone/bench.capstone.test.ts
---

## Project brief

You’ll build a **mini analytics pipeline** that ingests a stream of user events, normalizes them, dedupes by event id, computes a few metrics, and emits a **report** that your React UI could render.

We’ll intentionally use:
- **Arrays** for presentation & final materialization,
- **Map/Set** for uniqueness & fast lookup,
- **(Async)Iterables** for lazy streaming,
- **TypedArrays** for numeric series & zero-copy windows.

---

## Data model

Each event (NDJSON-ish):

```ts
export type RawEvent = {
  id: string;                 // may repeat (duplicates)
  ts: number;                 // epoch ms
  userId: string;
  type: "view" | "click" | "purchase";
  value?: number;             // e.g. amount for purchases
  tags?: string[];            // optional, noisy casing/spacing
  text?: string;              // optional, for anagram demo
};
````

After normalization:

```ts
export type Event = {
  id: string;
  ts: number;
  userId: string;
  type: "view" | "click" | "purchase";
  value: number;              // default 0 for non-purchase
  tags: string[];             // trimmed, lowercased
  text?: string;              // canonicalized (NFC, lower, no spaces/punct)
};
```

---

## Metrics to compute

1. **Top-K tags** overall (default K=10) — frequency table.
2. **Per-user totals**: views, clicks, purchases count, and sum of `value`.
3. **Sliding window** global **purchase moving average** over last `W` points (e.g., W=1000) using a numeric buffer (**Float32Array**).
4. **Anagram classes** among `text` (optional field): group strings that are anagrams (use the **frequency key** strategy).
5. **Daily activity**: group by UTC day (YYYY-MM-DD) into counts per type.

The **report** should expose both entry arrays and object/Map forms where useful.

---

## Architecture plan

1. **Ingest**: an `AsyncIterable<RawEvent>` (simulated generator in tests).
2. **Normalize**: `mapAsync` over the stream (trim/lower, NFC normalize).
3. **Dedupe**: last-win by `id` (retain the **latest** `ts` for repeated ids).
4. **Fork**: consume the deduped stream to compute metrics in one pass where possible:

   * update per-user counters (`Map<userId, Stats>`)
   * push purchases into a **Float32Array ring buffer** for moving average
   * accumulate tag counts (`Map<tag, number>`)
   * collect `text` anagram signature counts/groups
   * record daily counts (`Map<day, {view,click,purchase}>`)
5. **Materialize**: build final arrays for UI: `topTags[]`, `users[]`, `daily[]`.

---

## File scaffold & starters

Create a `capstone/` folder with these starters.

### 1) Data generator — `generateData.ts`

Produce a reproducible pseudo-random stream.

```ts
export async function* generateData(n: number, seed = 42): AsyncIterable<RawEvent> {
  let s = seed >>> 0;
  const rnd = () => (s = (s * 1664525 + 1013904223) >>> 0) / 2**32;

  const types = ["view","click","purchase"] as const;
  for (let i = 0; i < n; i++) {
    const type = types[(rnd() * types.length) | 0];
    const id = `e_${(rnd()*n)|0}`;               // collisions → duplicates
    const userId = `u_${(rnd()*1000)|0}`;        // up to 1k users
    const ts = Date.UTC(2025, 0, 1) + i * 1379;  // ~1.379s apart
    const words = ["Dormitory","Dirty room!!","résumé","sérumé","node","done"];
    const text = rnd() < 0.2 ? words[(rnd()*words.length)|0] : undefined;
    const tags = rnd() < 0.6 ? ["  Sports ", "tech", "Tech ", "news"][((rnd()*4)|0)]!.split(/\s*,\s*/).map(x=>x) : undefined;

    yield { id, ts, userId, type, value: type==="purchase" ? Math.round(rnd()*20000)/100 : undefined, tags, text };
    if (i % 20000 === 0) await Promise.resolve(); // yield
  }
}
```

### 2) Normalization — `normalize.ts`

```ts
export function normalizeTag(s: string): string {
  return s.trim().toLowerCase();
}

export function canonicalizeText(s: string): string {
  return s.normalize("NFC").toLowerCase().replace(/[\s\p{P}\p{S}]+/gu, "");
}

export function normalize(raw: import("./generateData").RawEvent): import("./generateData").Event {
  return {
    id: raw.id,
    ts: raw.ts,
    userId: raw.userId,
    type: raw.type,
    value: raw.type === "purchase" ? raw.value ?? 0 : 0,
    tags: (raw.tags ?? []).map(normalizeTag),
    text: raw.text ? canonicalizeText(raw.text) : undefined,
  };
}
```

### 3) Dedupe (last-win) — `dedupe.ts`

Streaming last-win by `id` using a **Map** of `id → index in buffer`. For the capstone, we’ll buffer a limited window then flush (simple approach), or accumulate all (OK for tests).

```ts
export async function* dedupeLastWin(stream: AsyncIterable<import("./generateData").Event>):
  AsyncIterable<import("./generateData").Event> {
  const last = new Map<string, import("./generateData").Event>();
  for await (const ev of stream) last.set(ev.id, ev);
  // Emit in insertion order of Map (last write wins)
  for (const e of last.values()) yield e;
}
```

*(For real pipelines you’d implement a bounded LRU or watermark-based flush; out of scope here.)*

### 4) Metrics — `metrics.ts`

Implement all metrics incrementally; keep **Maps** during accumulation; at the end, **materialize** entries arrays.

```ts
export type UserStats = { views: number; clicks: number; purchases: number; valueSum: number };
export type Daily = { day: string; views: number; clicks: number; purchases: number };
export type TopTag = [string, number];

export type AnagramBucket = { key: string; words: string[] };

export type Metrics = {
  topTags: TopTag[];
  users: Array<[string, UserStats]>;
  daily: Daily[];
  movingAvg: Float32Array; // last W purchase avgs, or empty if none
  anagrams: AnagramBucket[];
};

export async function computeMetrics(
  stream: AsyncIterable<import("./generateData").Event>,
  opts: { topK?: number; window?: number } = {}
): Promise<Metrics> {
  const topK = opts.topK ?? 10;
  const W = Math.max(1, opts.window ?? 1000);

  const tagCounts = new Map<string, number>();
  const user = new Map<string, UserStats>();
  const day = new Map<string, { v: number; c: number; p: number }>();
  const purchaseRing = new Float32Array(W);
  let ringSize = 0, ringWrite = 0, ringSum = 0;

  // Anagram via frequency key
  const buckets = new Map<string, string[]>();
  const freqKey = (t: string) => {
    const m = new Map<string, number>();
    for (const ch of t) m.set(ch, (m.get(ch) ?? 0) + 1);
    return [...m.entries()].sort((a,b)=>a[0].localeCompare(b[0])).map(([ch,n])=>ch+n).join("|");
  };

  for await (const e of stream) {
    // per-user
    const st = user.get(e.userId) ?? { views:0, clicks:0, purchases:0, valueSum:0 };
    if (e.type === "view") st.views++;
    else if (e.type === "click") st.clicks++;
    else { st.purchases++; st.valueSum += e.value; }
    user.set(e.userId, st);

    // tags
    for (const t of e.tags) tagCounts.set(t, (tagCounts.get(t) ?? 0) + 1);

    // daily
    const dayKey = new Date(e.ts).toISOString().slice(0,10);
    const d = day.get(dayKey) ?? { v:0,c:0,p:0 };
    if (e.type === "view") d.v++; else if (e.type === "click") d.c++; else d.p++;
    day.set(dayKey, d);

    // moving average ring (purchases only)
    if (e.type === "purchase") {
      const old = ringSize < W ? 0 : purchaseRing[ringWrite];
      if (ringSize < W) ringSize++;
      purchaseRing[ringWrite] = e.value;
      ringWrite = (ringWrite + 1) % W;
      ringSum += e.value - old;
    }

    // anagrams
    if (e.text) {
      const k = freqKey(e.text);
      (buckets.get(k) ?? buckets.set(k, []).get(k)!).push(e.text);
    }
  }

  // materialize
  const topTags = [...tagCounts].sort((a,b)=>b[1]-a[1]).slice(0, topK) as TopTag[];
  const users = [...user]; // entry array
  const daily: Daily[] = [...day].map(([key, v]) => ({ day: key, views: v.v, clicks: v.c, purchases: v.p }))
                                 .sort((a,b)=>a.day.localeCompare(b.day));
  const movingAvg = ringSize ? (() => {
    // Build the rolling average series aligned to ring content
    const out = new Float32Array(ringSize);
    // For simplicity, emit a single average at the end:
    out[out.length-1] = ringSum / ringSize;
    return out;
  })() : new Float32Array();

  const anagrams = [...buckets.values()]
    .filter(arr => arr.length > 1)
    .map(words => ({ key: words[0]!, words: Array.from(new Set(words)) }));

  return { topTags, users, daily, movingAvg, anagrams };
}
```

### 5) Report — `report.ts`

Turn metrics into a UI-friendly object.

```ts
export type Report = {
  summary: {
    users: number;
    totalPurchases: number;
    totalValue: number;
    topTag?: [string, number];
  };
  tables: {
    topTags: Array<{ tag: string; count: number }>;
    users: Array<{ userId: string; views: number; clicks: number; purchases: number; valueSum: number }>;
    daily: Array<{ day: string; views: number; clicks: number; purchases: number }>;
  };
  charts: {
    movingAvg: number[]; // materialized for chart lib
  };
  anagrams: Array<{ key: string; words: string[] }>;
};

export function buildReport(m: import("./metrics").Metrics): Report {
  const totalValue = m.users.reduce((s, [,st]) => s + st.valueSum, 0);
  const totalPurchases = m.users.reduce((s, [,st]) => s + st.purchases, 0);
  return {
    summary: {
      users: m.users.length,
      totalPurchases,
      totalValue,
      topTag: m.topTags[0],
    },
    tables: {
      topTags: m.topTags.map(([tag, count]) => ({ tag, count })),
      users: m.users.map(([userId, st]) => ({ userId, ...st })),
      daily: m.daily,
    },
    charts: {
      movingAvg: Array.from(m.movingAvg),
    },
    anagrams: m.anagrams,
  };
}
```

### 6) Bench (optional) — `bench.capstone.ts`

Use your harness from **L5 Performance** to compare two dedupe strategies:

* All-in-memory `Map` (above)
* First-win with left→right scan and `Set` (no overwrite)

Also compare **top-K via sort** vs **top-K via min-heap** for very large unique tag counts (heap is extra credit).

---

## End-to-end runner (for local exploration)

```ts
// capstone/index.ts
import { generateData } from "./generateData";
import { normalize } from "./normalize";
import { dedupeLastWin } from "./dedupe";
import { computeMetrics } from "./metrics";
import { buildReport } from "./report";

async function* pipeline(n: number) {
  for await (const r of generateData(n)) yield normalize(r);
}

(async () => {
  const deduped = dedupeLastWin(pipeline(200_000));
  const metrics = await computeMetrics(deduped, { topK: 10, window: 1000 });
  const report = buildReport(metrics);
  // feed `report` to UI; here just log summary
  console.log(report.summary);
})();
```

---

## Tests (what they verify)

* **generateData**: deterministic sizes for seed, rough distribution sanity.
* **normalize**: NFC, lowercase, tag trimming.
* **dedupe**: last-win semantics (latest `ts` kept).
* **metrics**: counts match small hand-built streams; movingAvg length/last value sane; anagram grouping correct.
* **report**: totals equal sums; top tag aligns; shapes stable.

---

## Stretch goals (pick any)

* Implement a **bounded dedupe** with an LRU of size `M` and a time watermark.
* Replace `topTags sort` with a **size-K min-heap** (good for huge `u`).
* Emit **per-user sliding windows** using `Map<userId, Float32Array>` ring buffers.
* Add a **stable partition** for events by type using extra space (compare to in-place unstable).
* Use `toSorted`/`toSpliced` immutably in report shaping; benchmark vs mutating equivalents.

---

## Quiz (checks)

1. The best structure for **last-win** dedupe by id is:
   A) `Set<string>` only
   B) **`Map<id, event>` overwriting on each sighting** ✅
   C) Array + `indexOf`
   D) Object with random deletion

2. Keeping a **moving average** of last `W` values efficiently uses:
   A) `concat` on a number\[] each time
   B) **Ring buffer (TypedArray) + rolling sum** ✅
   C) Sorting the last `W` values
   D) `reduce` across the entire history

3. For **Top-K tags** with large `u`, the scalable approach is:
   A) Sort all entries
   B) **Maintain a size-K min-heap** ✅
   C) Use `map(parseInt)`
   D) Double `K` until unique exhausted

4. Normalizing tags to ensure grouping should at least:
   A) Reverse each string
   B) **Trim + lowercase** ✅
   C) Uppercase only
   D) Deduplicate letters

5. Grouping anagrams **without sorting each word** can use:
   A) JSON.stringify on arrays
   B) **Character frequency key** ✅
   C) `includes` on substrings
   D) `localeCompare` on pairs

---

## Takeaways

* Keep streams **lazy**; only **materialize** for UI or cross-section joins.
* Use **Map/Set** for O(1) membership & accumulation; convert to **entries arrays** for sorting & display.
* TypedArray **ring buffers** plus rolling sums give O(1) moving averages.
* Be explicit about **policies** (last-win vs first-win, normalization rules).
* Benchmark choices that matter; pick clarity by default, speed when proven.

---

